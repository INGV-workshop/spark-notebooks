{
    "metadata": {
        "kernelspec": {
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21"
        }, 
        "language_info": {
            "name": "scala", 
            "codemirror_mode": "text/x-scala", 
            "version": "2.11.8", 
            "mimetype": "text/x-scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "execution_count": 1, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Starting download from https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.2/org.eclipse.paho.client.mqttv3-1.0.2.jar\nFinished download of org.eclipse.paho.client.mqttv3-1.0.2.jar\n"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "Waiting for a Spark session to start..."
                    }
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Starting download from http://central.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar\nFinished download of gson-2.2.4.jar\nStarting download from https://github.com/sathipal/spark-streaming-mqtt-with-security_2.10-1.3.0/releases/download/0.0.1/spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar\nFinished download of spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar\n"
                }
            ], 
            "source": "%AddJar https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.2/org.eclipse.paho.client.mqttv3-1.0.2.jar -f\n%AddJar http://central.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar -f\n%AddJar https://github.com/sathipal/spark-streaming-mqtt-with-security_2.10-1.3.0/releases/download/0.0.1/spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar -f"
        }, 
        {
            "execution_count": 2, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "import org.eclipse.paho.client.mqttv3._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.mqtt._\nimport org.apache.spark.SparkConf"
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "val ssc = new StreamingContext(sc.asInstanceOf[SparkContext], Seconds(1))\n\nval lines = MQTTUtils.createStream(ssc, // Spark Streaming Context\n\"ssl://3etv6p.messaging.internetofthings.ibmcloud.com:8883\", // Watson IoT Platform URL\n\"iot-2/type/+/id/+/evt/+/fmt/+\", // MQTT topic to receive the events\n\"a:3etv6p:random\", // Unique ID of the application\n\"a-3etv6p-7icb4uf5fh\", // API-Key\n\"wgbXafCw*!eR3x?lDm\") // Auth-Token"
        }, 
        {
            "execution_count": 4, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "import java.util.Map.Entry\nimport com.google.gson.JsonElement\nimport com.google.gson.JsonParser\nimport java.util.Set\nimport collection.mutable.HashMap"
        }, 
        {
            "execution_count": 5, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "val deviceMappedLines = lines.map(x => ((x.split(\" \", 2)(0)).split(\"/\")(4), x.split(\" \", 2)(1)))"
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "// Map the Json payload into scala map\nval jsonLines = deviceMappedLines.map(x => {\n var dataMap:scala.collection.mutable.Map[String, Any] = scala.collection.mutable.Map()\n val payload = new JsonParser().parse(x._2).getAsJsonObject()\n var deviceObject = payload\n val setObj = deviceObject.entrySet()\n val itr = setObj.iterator()\n while(itr.hasNext()) {\n val entry = itr.next();\n try {\n dataMap.put(entry.getKey(), entry.getValue().getAsDouble())\n } catch {\n case e: Exception => dataMap.put(entry.getKey(), entry.getValue().getAsString())\n }\n }\n (x._1, dataMap)\n})"
        }, 
        {
            "execution_count": 7, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": "val countEvMap = new HashMap[Tuple2[Int,Int],Int]()"
        }, 
        {
            "execution_count": 8, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.SparkException\nMessage: Task not serializable\nStackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n  at org.apache.spark.SparkContext.clean(SparkContext.scala:2107)\n  at org.apache.spark.streaming.dstream.DStream$$anonfun$filter$1.apply(DStream.scala:559)\n  at org.apache.spark.streaming.dstream.DStream$$anonfun$filter$1.apply(DStream.scala:559)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:709)\n  at org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\n  at org.apache.spark.streaming.dstream.DStream.filter(DStream.scala:558)\n  ... 46 elided\nCaused by: java.io.NotSerializableException: org.apache.spark.streaming.StreamingContext\nSerialization stack:\n\t- object not serializable (class: org.apache.spark.streaming.StreamingContext, value: org.apache.spark.streaming.StreamingContext@1291a834)\n\t- field (class: $iw, name: ssc, type: class org.apache.spark.streaming.StreamingContext)\n\t- object (class $iw, $iw@d5be7f7f)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@f033013b)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@d7f72093)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@662768e9)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@244a63ad)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@51d0de04)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@14203c51)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@624b2eaa)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@29ae0be2)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@590b165d)\n\t- field (class: $line30.$read, name: $iw, type: class $iw)\n\t- object (class $line30.$read, $line30.$read@dd1a12e2)\n\t- field (class: $iw, name: $line30$read, type: class $line30.$read)\n\t- object (class $iw, $iw@66f935ab)\n\t- field (class: $iw, name: $outer, type: class $iw)\n\t- object (class $iw, $iw@6b1c5e80)\n\t- field (class: $anonfun$1, name: $outer, type: class $iw)\n\t- object (class $anonfun$1, <function1>)\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "/**\n * Create a simple threshold rule. If coordinates are too large, alert\n */\n \n val threasholdCrossedLines = jsonLines.filter(\n x => {\n var status = false;\n val lat = x._2.get(\"latitude\").toString.toDouble\n val lon = x._2.get(\"longitude\").toString.toDouble\n if(lat != null && lon != null) {\n\t /* example */\n     val latInt = 100.0*lat;\n     val lonInt = 100.0*lon;\n     val t = new Tuple2[Int,Int](latInt.toInt,lonInt.toInt)\n     countEvMap.put(t, countEvMap.get(t).getOrElse(0) + 1)\n    status = true;\n\n }\n     status\n})"
        }, 
        {
            "execution_count": 9, 
            "metadata": {
                "scrolled": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Name: Unknown Error\nMessage: lastException: Throwable = null\n<console>:41: error: not found: value threasholdCrossedLines\n       threasholdCrossedLines.print()\n       ^\nStackTrace: "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "threasholdCrossedLines.print()\n\nssc.start()\nssc.awaitTermination()"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "source": ""
        }
    ], 
    "nbformat_minor": 1
}