{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "%AddJar https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.2/org.eclipse.paho.client.mqttv3-1.0.2.jar -f\n%AddJar http://central.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar -f\n%AddJar https://github.com/sathipal/spark-streaming-mqtt-with-security_2.10-1.3.0/releases/download/0.0.1/spark-streaming-mqtt-security_2.10-1.3.0-0.0.1.jar -f", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "import org.eclipse.paho.client.mqttv3._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.mqtt._\nimport org.apache.spark.SparkConf", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "val ssc = new StreamingContext(sc.asInstanceOf[SparkContext], Seconds(1))\n\nval lines = MQTTUtils.createStream(ssc, // Spark Streaming Context\n\"ssl://3etv6p.messaging.internetofthings.ibmcloud.com:8883\", // Watson IoT Platform URL\n\"iot-2/type/+/id/+/evt/+/fmt/+\", // MQTT topic to receive the events\n\"a:3etv6p:random\", // Unique ID of the application\n\"a-3etv6p-7icb4uf5fh\", // API-Key\n\"wgbXafCw*!eR3x?lDm\") // Auth-Token", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "import java.util.Map.Entry\nimport com.google.gson.JsonElement\nimport com.google.gson.JsonParser\nimport java.util.Set\nimport collection.mutable.HashMap", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "val deviceMappedLines = lines.map(x => ((x.split(\" \", 2)(0)).split(\"/\")(4), x.split(\" \", 2)(1)))", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "// Map the Json payload into scala map\nval jsonLines = deviceMappedLines.map(x => {\n var dataMap:scala.collection.mutable.Map[String, String] = scala.collection.mutable.Map()\n val payload = new JsonParser().parse(x._2).getAsJsonObject()\n var deviceObject = payload\n val setObj = deviceObject.entrySet()\n val itr = setObj.iterator()\n while(itr.hasNext()) {\n val entry = itr.next();\n try {\n dataMap.put(entry.getKey(), entry.getValue().toString)\n } catch {\n case e: Exception => dataMap.put(entry.getKey(), entry.getValue().toString)\n }\n }\n (x._1, dataMap)\n})", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "val cellsCoordinates = jsonLines.map(x => {\n val lat = x._2.get(\"latitude\").get.toString.toFloat\n val lon = x._2.get(\"longitude\").get.toString.toFloat\n val latInt = 100.0*lat;\n val lonInt = 100.0*lon;\n (latInt.toInt,lonInt.toInt)  \n })\n\nval countCC = cellsCoordinates.countByValue()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "countCC.print()\n\nssc.start()\nssc.awaitTermination()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "scala-spark21", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1"
        }, 
        "language_info": {
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala"
        }
    }
}